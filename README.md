# 🧠 Multimodal AI: Image + Language Caption Matcher

## 🚀 AI That Sees and Speaks

A deep learning project by **Kapil Tanwar** that builds a **multimodal system** using **CLIP** (Contrastive Language-Image Pretraining).  
The model takes an image input and ranks a list of 70+ caption candidates based on **cosine similarity**, returning the **Top 5 best matching captions**.

---

### 📸 What It Does

- Upload an image (like a cup of tea)
- Compares it to a diverse list of captions
- Uses **CLIP’s image and text embeddings** to calculate similarity
- Returns the most relevant captions (ranked by semantic similarity)

---

### 🛠️ Tech Stack

- Python 🐍
- PyTorch ⚙️
- Hugging Face Transformers 🤗
- OpenAI CLIP model 🖼️
- Scikit-learn (cosine similarity)
- Google Colab (for easy file upload & execution)

---

### ✨ Applications
🔍 Visual Search

- 🛒 E-commerce product tagging

- 📱 Social media post assistants

- 🤖 Captioning for AI agents

- 🧠 Generative storytelling & prompt starters

### 📁 Project Files
File	Description
building_a_multimodal_ai_model_with_python.ipynb Full working code for the project
building_a_multimodal_ai_model_with_python.py	Full working code for the project
Kapil.png (example)	Sample image input
README.md	You’re reading it 👀




# 🧠 Author
## Kapil Tanwar
### Data Scientist | AI/ML Engineer | Storyteller of Code & Vision
- I love building systems where vision meets language.

