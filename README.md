# ğŸ§  Multimodal AI: Image + Language Caption Matcher

## ğŸš€ AI That Sees and Speaks

A deep learning project by **Kapil Tanwar** that builds a **multimodal system** using **CLIP** (Contrastive Language-Image Pretraining).  
The model takes an image input and ranks a list of 70+ caption candidates based on **cosine similarity**, returning the **Top 5 best matching captions**.

---

### ğŸ“¸ What It Does

- Upload an image (like a cup of tea)
- Compares it to a diverse list of captions
- Uses **CLIPâ€™s image and text embeddings** to calculate similarity
- Returns the most relevant captions (ranked by semantic similarity)

---

### ğŸ› ï¸ Tech Stack

- Python ğŸ
- PyTorch âš™ï¸
- Hugging Face Transformers ğŸ¤—
- OpenAI CLIP model ğŸ–¼ï¸
- Scikit-learn (cosine similarity)
- Google Colab (for easy file upload & execution)

---

### âœ¨ Applications
ğŸ” Visual Search

- ğŸ›’ E-commerce product tagging

- ğŸ“± Social media post assistants

- ğŸ¤– Captioning for AI agents

- ğŸ§  Generative storytelling & prompt starters

### ğŸ“ Project Files
File	Description
building_a_multimodal_ai_model_with_python.ipynb Full working code for the project
building_a_multimodal_ai_model_with_python.py	Full working code for the project
Kapil.png (example)	Sample image input
README.md	Youâ€™re reading it ğŸ‘€




# ğŸ§  Author
## Kapil Tanwar
### Data Scientist | AI/ML Engineer | Storyteller of Code & Vision
- I love building systems where vision meets language.

